{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Copy of hackaton.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "kpmSbtrCklRB",
        "colab_type": "code",
        "outputId": "dbc45e00-7cbd-48f5-d653-387f02b4beaa",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 121
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lNLhWWCxYfLf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import re\n",
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import f1_score\n",
        "import scipy\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import logging\n",
        "import multiprocessing\n",
        "import gensim\n",
        "from gensim.models import Word2Vec\n",
        "from sklearn.decomposition import PCA"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z8IzA7Rx-VzU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.layers import Dense, Input, LSTM, Embedding, Dropout, Activation, Conv1D\n",
        "from tensorflow.keras import Sequential\n",
        "from tensorflow.keras.layers import Bidirectional, GlobalMaxPooling1D, GlobalMaxPool1D, MaxPooling1D, Flatten, concatenate\n",
        "from tensorflow.keras.models import Model, load_model\n",
        "from tensorflow.keras import initializers, regularizers, constraints, optimizers, layers\n",
        "from tensorflow.keras.callbacks import CSVLogger, ModelCheckpoint, EarlyStopping\n",
        "\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from tensorflow.keras import backend as K"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Oc94smEhjbA_",
        "colab_type": "text"
      },
      "source": [
        "# Download data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "75potnAshWU4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "data_path = '/content/drive/My Drive/Colab Notebooks/Hackathon/'\n",
        "wili_data_path = data_path + 'wili-2018/'\n",
        "\n",
        "x_train_path = wili_data_path + 'x_train.txt'\n",
        "x_test_path = wili_data_path + 'x_test.txt'\n",
        "y_train_path = wili_data_path + 'y_train.txt'\n",
        "y_test_path = wili_data_path + 'y_test.txt'\n",
        "labels_path = wili_data_path + 'labels.csv'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NhsiujWS3QR8",
        "colab_type": "code",
        "outputId": "3036ba7d-8e22-47e5-d77f-a53fe3261b10",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 402
        }
      },
      "source": [
        "labels = pd.read_csv(labels_path, sep=';')\n",
        "labels = labels.drop(labels=['German', 'Writing system', 'Remarks', 'Synonyms'], axis=1)\n",
        "labels"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Label</th>\n",
              "      <th>English</th>\n",
              "      <th>Wiki Code</th>\n",
              "      <th>ISO 369-3</th>\n",
              "      <th>Language family</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>ace</td>\n",
              "      <td>Achinese</td>\n",
              "      <td>ace</td>\n",
              "      <td>ace</td>\n",
              "      <td>Austronesian</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>afr</td>\n",
              "      <td>Afrikaans</td>\n",
              "      <td>af</td>\n",
              "      <td>afr</td>\n",
              "      <td>Indo-European</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>als</td>\n",
              "      <td>Alemannic German</td>\n",
              "      <td>als</td>\n",
              "      <td>gsw</td>\n",
              "      <td>Indo-European</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>amh</td>\n",
              "      <td>Amharic</td>\n",
              "      <td>am</td>\n",
              "      <td>amh</td>\n",
              "      <td>Afro-Asiatic</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>ang</td>\n",
              "      <td>Old English</td>\n",
              "      <td>ang</td>\n",
              "      <td>ang</td>\n",
              "      <td>Indo-European</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>230</th>\n",
              "      <td>yid</td>\n",
              "      <td>Yiddish</td>\n",
              "      <td>yi</td>\n",
              "      <td>yid</td>\n",
              "      <td>Indo-European</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>231</th>\n",
              "      <td>yor</td>\n",
              "      <td>Yoruba</td>\n",
              "      <td>yo</td>\n",
              "      <td>yor</td>\n",
              "      <td>Niger-Congo</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>232</th>\n",
              "      <td>zea</td>\n",
              "      <td>Zeeuws</td>\n",
              "      <td>zea</td>\n",
              "      <td>zea</td>\n",
              "      <td>Indo-European</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>233</th>\n",
              "      <td>zh-yue</td>\n",
              "      <td>Cantonese</td>\n",
              "      <td>zh-yue</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Sino-Tibetan</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>234</th>\n",
              "      <td>zho</td>\n",
              "      <td>Standard Chinese</td>\n",
              "      <td>zh</td>\n",
              "      <td>zho</td>\n",
              "      <td>Sino-Tibetan</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>235 rows × 5 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "      Label           English Wiki Code ISO 369-3 Language family\n",
              "0       ace          Achinese       ace       ace    Austronesian\n",
              "1       afr         Afrikaans        af       afr   Indo-European\n",
              "2       als  Alemannic German       als       gsw   Indo-European\n",
              "3       amh           Amharic        am       amh    Afro-Asiatic\n",
              "4       ang      Old English        ang       ang   Indo-European\n",
              "..      ...               ...       ...       ...             ...\n",
              "230     yid           Yiddish        yi       yid   Indo-European\n",
              "231     yor            Yoruba        yo       yor     Niger-Congo\n",
              "232     zea            Zeeuws       zea       zea   Indo-European\n",
              "233  zh-yue         Cantonese    zh-yue       NaN    Sino-Tibetan\n",
              "234     zho  Standard Chinese        zh       zho    Sino-Tibetan\n",
              "\n",
              "[235 rows x 5 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vt7kVtUgYRi8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# convert title of language in class\n",
        "with open(y_train_path) as f:\n",
        "    mylist = f.read().splitlines() \n",
        "languages = list(set(mylist))\n",
        "language_to_class = dict()\n",
        "class_to_language = dict()\n",
        "for i in range(len(languages)):\n",
        "    language_to_class[languages[i]] = i\n",
        "    class_to_language[i] = languages[i]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UDFr3-rKmDdv",
        "colab_type": "code",
        "outputId": "8fa7c28f-52d4-43df-dba0-f2f50f13fd98",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        }
      },
      "source": [
        "print(list(language_to_class.items())[:5])\n",
        "len(languages)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[('pcd', 0), ('sme', 1), ('mhr', 2), ('bak', 3), ('est', 4)]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "235"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YBqkjck1Yc7q",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def read_data():\n",
        "    with open(x_train_path) as f:\n",
        "        mylist = f.read().splitlines() \n",
        "    x_train = pd.DataFrame(mylist, columns=[\"sent\"])\n",
        "    with open(x_test_path) as f:\n",
        "        mylist = f.read().splitlines() \n",
        "    x_test = pd.DataFrame(mylist, columns=[\"sent\"])\n",
        "\n",
        "    with open(y_train_path) as f:\n",
        "        mylist = f.read().splitlines() \n",
        "    y_train = []\n",
        "    for lan in mylist:\n",
        "        y_train.append(language_to_class[lan])\n",
        "    y_train = pd.DataFrame(y_train)\n",
        "    with open(y_test_path) as f:\n",
        "        mylist = f.read().splitlines()\n",
        "\n",
        "    y_test = []\n",
        "    for lan in mylist:\n",
        "        y_test.append(language_to_class[lan])\n",
        "    y_test = pd.DataFrame(y_test)\n",
        "    \n",
        "    return (x_train, y_train), (x_test, y_test)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lRo0Wf1lZXsE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "(x_train, y_train), (x_test, y_test) = read_data()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zhKRW6V8hhIc",
        "colab_type": "code",
        "outputId": "ba32dd28-2f27-4472-cb3b-1a97d91fc14f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 252
        }
      },
      "source": [
        "print(x_train.head())\n",
        "print(y_train.head())\n",
        "\n",
        "print(x_train.shape)\n",
        "print(x_test.shape)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "                                                sent\n",
            "0  Klement Gottwaldi surnukeha palsameeriti ning ...\n",
            "1  Sebes, Joseph; Pereira Thomas (1961) (på eng)....\n",
            "2  भारतीय स्वातन्त्र्य आन्दोलन राष्ट्रीय एवम क्षे...\n",
            "3  Après lo cort periòde d'establiment a Basilèa,...\n",
            "4  ถนนเจริญกรุง (อักษรโรมัน: Thanon Charoen Krung...\n",
            "     0\n",
            "0    4\n",
            "1  174\n",
            "2   91\n",
            "3   52\n",
            "4   77\n",
            "(117500, 1)\n",
            "(117500, 1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9zcUciRigFYu",
        "colab_type": "code",
        "outputId": "e8052582-725a-43b2-da4e-f3bc53817dec",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 265
        }
      },
      "source": [
        "lens = x_train.sent.str.split(' ').str.len().values\n",
        "plt.hist(lens, bins=np.linspace(0,1000,20), facecolor='blue', alpha=0.9)\n",
        "plt.show()"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4xdCSiHnhlX1",
        "colab_type": "text"
      },
      "source": [
        "# Experiment on small dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sFTqOQmChknq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# small_dataset = pd.read_csv(data_path + 'kaggle_dataset/dataset.csv')\n",
        "# small_dataset"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LwtWy38ehtGO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# X=small_dataset['Text']\n",
        "# y=small_dataset['language']\n",
        "\n",
        "# X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# print(len(X_train))\n",
        "# print(len(X_test))\n",
        "# print(len(y_train))\n",
        "# print(len(y_test))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XNoYz_IwjeY_",
        "colab_type": "text"
      },
      "source": [
        "# Embedding **data**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bEr8pklSjQLP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "filter_out = '!\"#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n1234567890'\n",
        "tokenizer = Tokenizer(filters=filter_out, lower=True)\n",
        "tokenizer.fit_on_texts(x_train.sent)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a5jxVwDcftcn",
        "colab_type": "text"
      },
      "source": [
        "Выбираю какое количество слов оставить. Если оставлять все, то будет 1500000, что слишком много. Можно поменять `count` и оно покажет сколько слов встречается больше, чем `count` раз"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4xKQt34TAG2-",
        "colab_type": "code",
        "outputId": "956615e7-f200-48b8-d73d-16b61d88e45e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# try different values of `count`\n",
        "# show how many words are presenting more than `count` times\n",
        "count = 10\n",
        "frequent_words = [w for w,c in tokenizer.word_counts.items() if c > count]\n",
        "len(frequent_words)"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "64157"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QJacuBHhHEng",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "embed_size = 300 # how big is each word vector\n",
        "max_features = len(frequent_words) # how many unique words to use (i.e num rows in embedding vector)\n",
        "maxlen = 200 # max number of words in a question to use\n",
        "\n",
        "## Tokenize the sentences\n",
        "tokenizer = Tokenizer(num_words=max_features, filters=filter_out, lower=True)\n",
        "\n",
        "def prepare_data():\n",
        "    x_tr, x_val, y_tr, y_val = train_test_split(x_train, y_train, test_size=0.1, random_state=1, stratify=y_train.values)\n",
        "   \n",
        "    tokenizer.fit_on_texts(x_tr.sent)\n",
        "\n",
        "    train_X = tokenizer.texts_to_sequences(x_tr.sent)\n",
        "    val_X = tokenizer.texts_to_sequences(x_val.sent)\n",
        "\n",
        "    ## Pad the sentences \n",
        "    train_X = pad_sequences(train_X, maxlen=maxlen)\n",
        "    val_X = pad_sequences(val_X, maxlen=maxlen)\n",
        "\n",
        "    return (train_X, y_tr), (val_X, y_val)\n",
        "    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qW1oK9wfHEnm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "(train_X, train_Y), (val_X, val_Y) = prepare_data()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HxyyqzKsgF9K",
        "colab_type": "text"
      },
      "source": [
        "Проверила, что в тренировочном и валидационном датасетах встречаются все классы. Балансировка регулируется вот этим параметром: stratify=y_train.values"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R1iJF7cyU5iK",
        "colab_type": "code",
        "outputId": "9737d09d-098a-4fbb-9ef2-6d5c8b1e1ac1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "len(set(val_Y.values.reshape(1, -1)[0]))"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "235"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aPITYtR8UqNx",
        "colab_type": "code",
        "outputId": "267aa1d0-e840-4acd-ae21-57d6227334da",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "len(set(train_Y.values.reshape(1, -1)[0]))"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "235"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TYuurOSQgYUl",
        "colab_type": "text"
      },
      "source": [
        "Ниже меняю представление векторов ответов из [1, 23, 10,...] в вектор длины 235 и с 1 на месте соответсвующего языка"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b0iIyf4bIXg_",
        "colab_type": "code",
        "outputId": "f84d4e6c-56f5-4d5d-f32a-006c0bf73fbc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 104
        }
      },
      "source": [
        "lb = LabelEncoder()\n",
        "y = lb.fit_transform(train_Y.values)\n",
        "dummy_y_train = to_categorical(y)\n",
        "print(len(dummy_y_train))\n",
        "print(len(dummy_y_train[0]))"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "105750\n",
            "235\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/preprocessing/_label.py:251: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
            "  y = column_or_1d(y, warn=True)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vCBYAruyIxUm",
        "colab_type": "code",
        "outputId": "abbab09d-7d7e-4c4d-f085-b6ca77600e39",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        }
      },
      "source": [
        "y = lb.fit_transform(val_Y.values)\n",
        "dummy_y_val = to_categorical(y)"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/preprocessing/_label.py:251: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
            "  y = column_or_1d(y, warn=True)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MeXXgZJ6miyl",
        "colab_type": "text"
      },
      "source": [
        "# Metrics"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vp4rfHfplm6O",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def f1(y_true, y_pred): #taken from old keras source code\n",
        "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
        "    possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
        "    predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
        "    precision = true_positives / (predicted_positives + K.epsilon())\n",
        "    recall = true_positives / (possible_positives + K.epsilon())\n",
        "    f1_val = 2*(precision*recall)/(precision+recall+K.epsilon())\n",
        "    return f1_val"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Dzi3zyNsmNIR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def recall(y_true, y_pred): #taken from old keras source code\n",
        "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
        "    possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
        "    predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
        "    return true_positives / (possible_positives + K.epsilon())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JX13CaozmRhy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def precision(y_true, y_pred): #taken from old keras source code\n",
        "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
        "    possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
        "    predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
        "    return true_positives / (predicted_positives + K.epsilon())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VtGH8P89gTXf",
        "colab_type": "text"
      },
      "source": [
        "#CNN"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vdsE9d9SjdsW",
        "colab_type": "code",
        "outputId": "73c85e82-c0ca-43dc-a835-071908d148c5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 487
        }
      },
      "source": [
        "cnn_model = Sequential()\n",
        "cnn_model.add(Embedding(max_features, embed_size,input_shape=(maxlen,)))\n",
        "cnn_model.add(Conv1D(256, 5, activation='relu', input_shape=(embed_size,)))\n",
        "cnn_model.add(MaxPooling1D(5))\n",
        "cnn_model.add(Dropout(0.2))\n",
        "cnn_model.add(Conv1D(256, 5, activation='relu'))\n",
        "cnn_model.add(MaxPooling1D(5))\n",
        "cnn_model.add(Flatten())\n",
        "cnn_model.add(Dense(512, activation=\"relu\"))\n",
        "cnn_model.add(Dropout(0.2))\n",
        "cnn_model.add(Dense(len(languages), activation='softmax'))\n",
        "cnn_model.summary()"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding (Embedding)        (None, 200, 300)          19247100  \n",
            "_________________________________________________________________\n",
            "conv1d (Conv1D)              (None, 196, 256)          384256    \n",
            "_________________________________________________________________\n",
            "max_pooling1d (MaxPooling1D) (None, 39, 256)           0         \n",
            "_________________________________________________________________\n",
            "dropout (Dropout)            (None, 39, 256)           0         \n",
            "_________________________________________________________________\n",
            "conv1d_1 (Conv1D)            (None, 35, 256)           327936    \n",
            "_________________________________________________________________\n",
            "max_pooling1d_1 (MaxPooling1 (None, 7, 256)            0         \n",
            "_________________________________________________________________\n",
            "flatten (Flatten)            (None, 1792)              0         \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (None, 512)               918016    \n",
            "_________________________________________________________________\n",
            "dropout_1 (Dropout)          (None, 512)               0         \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 235)               120555    \n",
            "=================================================================\n",
            "Total params: 20,997,863\n",
            "Trainable params: 20,997,863\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Sndko3XlZ806",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "cnn_model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy', f1, recall, precision])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Aa9rf4_Oq6nX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "cnn_cp_path = data_path+'model_cnn.hdf5'\n",
        "cnn_cp=ModelCheckpoint(cnn_cp_path, monitor='val_f1',save_best_only=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LIhz76oFHo_c",
        "colab_type": "code",
        "outputId": "34ef75e4-cb89-4d30-bc37-9c7dbaa954a3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 222
        }
      },
      "source": [
        "cnn_model.fit(train_X, dummy_y_train, batch_size=512, epochs=5, \n",
        "              validation_data=(val_X, dummy_y_val),\n",
        "              callbacks = [cnn_cp])"
      ],
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/5\n",
            "207/207 [==============================] - 58s 282ms/step - loss: 3.5370 - accuracy: 0.2596 - f1: 0.1976 - recall: 0.1347 - precision: 0.6100 - val_loss: 0.9810 - val_accuracy: 0.7858 - val_f1: 0.7683 - val_recall: 0.6403 - val_precision: 0.9607\n",
            "Epoch 2/5\n",
            "207/207 [==============================] - 56s 271ms/step - loss: 0.7851 - accuracy: 0.8039 - f1: 0.8147 - recall: 0.7301 - precision: 0.9247 - val_loss: 0.5844 - val_accuracy: 0.8709 - val_f1: 0.8936 - val_recall: 0.8367 - val_precision: 0.9590\n",
            "Epoch 3/5\n",
            "207/207 [==============================] - 56s 270ms/step - loss: 0.3818 - accuracy: 0.8991 - f1: 0.9144 - recall: 0.8713 - precision: 0.9621 - val_loss: 0.5681 - val_accuracy: 0.8803 - val_f1: 0.9013 - val_recall: 0.8580 - val_precision: 0.9493\n",
            "Epoch 4/5\n",
            "207/207 [==============================] - 55s 265ms/step - loss: 0.2586 - accuracy: 0.9289 - f1: 0.9412 - recall: 0.9105 - precision: 0.9740 - val_loss: 0.6221 - val_accuracy: 0.8820 - val_f1: 0.9042 - val_recall: 0.8676 - val_precision: 0.9441\n",
            "Epoch 5/5\n",
            "207/207 [==============================] - 55s 265ms/step - loss: 0.2059 - accuracy: 0.9406 - f1: 0.9523 - recall: 0.9274 - precision: 0.9786 - val_loss: 0.6610 - val_accuracy: 0.8869 - val_f1: 0.9058 - val_recall: 0.8741 - val_precision: 0.9399\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7fb187baa9e8>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 50
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AkmpFZmyUjF8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "dependencies = {\n",
        "     'f1': f1,\n",
        "     'recall': recall,\n",
        "     'precision': precision\n",
        "}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qji8xqW2WiS-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# cnn_model = load_model(cnn_cp_path, custom_objects=dependencies)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e-T-4MO1ZR_S",
        "colab_type": "text"
      },
      "source": [
        "#RNN"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7ch0GFZ-ZSzb",
        "colab_type": "code",
        "outputId": "d21db2f0-5845-4b46-e09f-74f007fd208f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 454
        }
      },
      "source": [
        "rnn_model = Sequential()\n",
        "rnn_model.add(Embedding(max_features, embed_size))\n",
        "rnn_model.add(Bidirectional(LSTM(256, return_sequences=True, activation='tanh',input_dim=embed_size)))\n",
        "rnn_model.add(Dropout(0.2))\n",
        "rnn_model.add(Bidirectional(LSTM(256, return_sequences=True, activation='tanh')))\n",
        "rnn_model.add(GlobalMaxPool1D())\n",
        "rnn_model.add(Dropout(0.2))\n",
        "rnn_model.add(Dense(512, activation=\"relu\"))\n",
        "rnn_model.add(Dropout(0.2))\n",
        "rnn_model.add(Dense(len(languages), activation='softmax'))\n",
        "rnn_model.summary()"
      ],
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_1 (Embedding)      (None, None, 300)         19247100  \n",
            "_________________________________________________________________\n",
            "bidirectional (Bidirectional (None, None, 512)         1140736   \n",
            "_________________________________________________________________\n",
            "dropout_2 (Dropout)          (None, None, 512)         0         \n",
            "_________________________________________________________________\n",
            "bidirectional_1 (Bidirection (None, None, 512)         1574912   \n",
            "_________________________________________________________________\n",
            "global_max_pooling1d (Global (None, 512)               0         \n",
            "_________________________________________________________________\n",
            "dropout_3 (Dropout)          (None, 512)               0         \n",
            "_________________________________________________________________\n",
            "dense_2 (Dense)              (None, 512)               262656    \n",
            "_________________________________________________________________\n",
            "dropout_4 (Dropout)          (None, 512)               0         \n",
            "_________________________________________________________________\n",
            "dense_3 (Dense)              (None, 235)               120555    \n",
            "=================================================================\n",
            "Total params: 22,345,959\n",
            "Trainable params: 22,345,959\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C1W3XFHXZiVR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "rnn_model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy', f1, recall, precision])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tPtLB7kUrGgE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "rnn_cp_path = data_path + 'model_rnn.hdf5'\n",
        "rnn_cp=ModelCheckpoint(rnn_cp_path,monitor='val_f1',save_best_only=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2qK-tgijZk7L",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# rnn_model.fit(train_X, dummy_y_train, batch_size=512, epochs=3, \n",
        "#               validation_data=(val_X, dummy_y_val), callbacks = [rnn_cp])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4kk3g4PLUumt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "rnn_model = load_model(rnn_cp_path, custom_objects=dependencies)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dCeuUze_qnC2",
        "colab_type": "text"
      },
      "source": [
        "# Results"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3jS0Tq-Cqb4t",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "x_test['sc'] = y_test\n",
        "x_train['sc'] = y_train"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wowtwiinqwm0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Be careful! Tokenizer and maxlen is taken from train dataset from 1st part\n",
        "def convert(x):\n",
        "    train_X = tokenizer.texts_to_sequences(x.sent)\n",
        "    ## Pad the sentences \n",
        "    train_X = pad_sequences(train_X, maxlen=maxlen)\n",
        "    return train_X"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WvOMHL7Nx_eO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def metr(y_true, y_pred): #taken from old keras source code\n",
        "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
        "    possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
        "    predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
        "    precision = true_positives / (predicted_positives + K.epsilon())\n",
        "    recall = true_positives / (possible_positives + K.epsilon())\n",
        "    f1_val = 2*(precision*recall)/(precision+recall+K.epsilon())\n",
        "    return precision, recall, f1_val"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-OwT-3obvsIo",
        "colab_type": "text"
      },
      "source": [
        "# Test"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P006Z5Sgzeal",
        "colab_type": "code",
        "outputId": "4e0500f9-fcfa-4a12-f68c-3e29a1fbcfe1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "ps = []\n",
        "rs = []\n",
        "fs = []\n",
        "for lan in range(len(languages)):\n",
        "    x_t = x_test[x_test['sc'] == lan]\n",
        "    x = convert(x_t)\n",
        "    y = np.zeros((x.shape[0], len(languages)))\n",
        "    y[:, lan] = 1\n",
        "    y = y.astype('float32')\n",
        "    y_pred = cnn_model.predict(x)\n",
        "    p, r, f = metr(y, y_pred)\n",
        "    ps.append(p.numpy())\n",
        "    rs.append(r.numpy())\n",
        "    fs.append(f.numpy())\n",
        "    print(\"language: \", class_to_language[lan], \"precision: \", p.numpy(), \"recall: \", r.numpy(), \"f1: \", f.numpy())"
      ],
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "language:  pcd precision:  0.7783019 recall:  0.66 f1:  0.7142857\n",
            "language:  sme precision:  0.9490446 recall:  0.894 f1:  0.92070025\n",
            "language:  mhr precision:  0.93958336 recall:  0.902 f1:  0.9204081\n",
            "language:  bak precision:  0.9634888 recall:  0.95 f1:  0.9566968\n",
            "language:  est precision:  0.9529148 recall:  0.85 f1:  0.89852\n",
            "language:  lao precision:  0.9235669 recall:  0.58 f1:  0.7125307\n",
            "language:  kin precision:  0.93868923 recall:  0.888 f1:  0.9126413\n",
            "language:  crh precision:  0.93670887 recall:  0.888 f1:  0.91170424\n",
            "language:  wln precision:  0.97352344 recall:  0.956 f1:  0.96468204\n",
            "language:  cbk precision:  0.78251123 recall:  0.698 f1:  0.7378435\n",
            "language:  kaz precision:  0.97703546 recall:  0.936 f1:  0.9560776\n",
            "language:  ile precision:  0.92511016 recall:  0.84 f1:  0.8805031\n",
            "language:  tam precision:  0.9932886 recall:  0.888 f1:  0.93769795\n",
            "language:  lrc precision:  0.93495935 recall:  0.92 f1:  0.9274193\n",
            "language:  hun precision:  0.9728601 recall:  0.932 f1:  0.9519918\n",
            "language:  pan precision:  1.0 recall:  0.982 f1:  0.9909182\n",
            "language:  lmo precision:  0.90554416 recall:  0.882 f1:  0.8936169\n",
            "language:  roa-tara precision:  0.95884776 recall:  0.932 f1:  0.9452333\n",
            "language:  guj precision:  0.9794239 recall:  0.952 f1:  0.9655172\n",
            "language:  pam precision:  0.9583333 recall:  0.92 f1:  0.9387754\n",
            "language:  wol precision:  0.9544513 recall:  0.922 f1:  0.937945\n",
            "language:  mar precision:  0.9665971 recall:  0.926 f1:  0.94586307\n",
            "language:  yor precision:  0.90707964 recall:  0.82 f1:  0.86134446\n",
            "language:  khm precision:  0.8625 recall:  0.414 f1:  0.55945945\n",
            "language:  kom precision:  0.87789476 recall:  0.834 f1:  0.8553845\n",
            "language:  kok precision:  0.91885966 recall:  0.838 f1:  0.876569\n",
            "language:  ilo precision:  0.97741276 recall:  0.952 f1:  0.964539\n",
            "language:  fin precision:  0.9476988 recall:  0.906 f1:  0.92638034\n",
            "language:  hak precision:  0.9748428 recall:  0.93 f1:  0.9518935\n",
            "language:  be-tarask precision:  0.686722 recall:  0.662 f1:  0.6741344\n",
            "language:  ltz precision:  0.9450317 recall:  0.894 f1:  0.91880774\n",
            "language:  bjn precision:  0.92402464 recall:  0.9 f1:  0.911854\n",
            "language:  srp precision:  0.9385593 recall:  0.886 f1:  0.9115226\n",
            "language:  kat precision:  0.9978992 recall:  0.95 f1:  0.9733606\n",
            "language:  oss precision:  0.97530866 recall:  0.948 f1:  0.9614604\n",
            "language:  jav precision:  0.9466119 recall:  0.922 f1:  0.93414384\n",
            "language:  ltg precision:  0.94957983 recall:  0.904 f1:  0.9262295\n",
            "language:  pus precision:  0.97125256 recall:  0.946 f1:  0.9584599\n",
            "language:  hsb precision:  0.94827586 recall:  0.88 f1:  0.91286296\n",
            "language:  stq precision:  0.9628866 recall:  0.934 f1:  0.9482233\n",
            "language:  rue precision:  0.8898305 recall:  0.84 f1:  0.8641975\n",
            "language:  lav precision:  0.9438445 recall:  0.874 f1:  0.90758044\n",
            "language:  tsn precision:  0.9815195 recall:  0.956 f1:  0.9685916\n",
            "language:  hau precision:  0.9736842 recall:  0.962 f1:  0.96780676\n",
            "language:  bod precision:  0.94382024 recall:  0.168 f1:  0.28522918\n",
            "language:  lug precision:  0.9665272 recall:  0.924 f1:  0.94478524\n",
            "language:  pdc precision:  0.8574468 recall:  0.806 f1:  0.8309277\n",
            "language:  swa precision:  0.9811321 recall:  0.936 f1:  0.9580347\n",
            "language:  msa precision:  0.86157024 recall:  0.834 f1:  0.8475609\n",
            "language:  sna precision:  0.94065934 recall:  0.856 f1:  0.896335\n",
            "language:  zea precision:  0.8340164 recall:  0.814 f1:  0.8238865\n",
            "language:  dty precision:  0.789899 recall:  0.782 f1:  0.7859296\n",
            "language:  oci precision:  0.89361703 recall:  0.84 f1:  0.8659793\n",
            "language:  nap precision:  0.92197126 recall:  0.898 f1:  0.9098277\n",
            "language:  nan precision:  0.9919355 recall:  0.984 f1:  0.9879518\n",
            "language:  ace precision:  0.9716599 recall:  0.96 f1:  0.9657947\n",
            "language:  ast precision:  0.85376346 recall:  0.794 f1:  0.8227979\n",
            "language:  pag precision:  0.9125 recall:  0.876 f1:  0.8938775\n",
            "language:  sun precision:  0.95454544 recall:  0.924 f1:  0.9390243\n",
            "language:  bre precision:  0.9756098 recall:  0.96 f1:  0.96774185\n",
            "language:  nrm precision:  0.94534415 recall:  0.934 f1:  0.9396377\n",
            "language:  arz precision:  0.94929004 recall:  0.936 f1:  0.9425981\n",
            "language:  krc precision:  0.9692623 recall:  0.946 f1:  0.9574898\n",
            "language:  lez precision:  0.96025103 recall:  0.918 f1:  0.9386502\n",
            "language:  glg precision:  0.8523207 recall:  0.808 f1:  0.8295687\n",
            "language:  pfl precision:  0.8794926 recall:  0.832 f1:  0.85508734\n",
            "language:  lad precision:  0.90208334 recall:  0.866 f1:  0.8836734\n",
            "language:  slv precision:  0.9223947 recall:  0.832 f1:  0.87486845\n",
            "language:  bul precision:  0.94572026 recall:  0.906 f1:  0.925434\n",
            "language:  eus precision:  0.9628866 recall:  0.934 f1:  0.9482233\n",
            "language:  xho precision:  0.95074946 recall:  0.888 f1:  0.918304\n",
            "language:  ext precision:  0.91004187 recall:  0.87 f1:  0.8895705\n",
            "language:  bel precision:  0.8912134 recall:  0.852 f1:  0.8711656\n",
            "language:  kan precision:  0.9497908 recall:  0.908 f1:  0.9284253\n",
            "language:  srd precision:  0.9642105 recall:  0.916 f1:  0.93948716\n",
            "language:  nob precision:  0.90833336 recall:  0.872 f1:  0.8897959\n",
            "language:  snd precision:  0.989899 recall:  0.98 f1:  0.98492455\n",
            "language:  tha precision:  0.9638989 recall:  0.534 f1:  0.68725866\n",
            "language:  map-bms precision:  0.8113208 recall:  0.774 f1:  0.79222107\n",
            "language:  vro precision:  0.9306723 recall:  0.886 f1:  0.9077868\n",
            "language:  ces precision:  0.93991417 recall:  0.876 f1:  0.9068322\n",
            "language:  urd precision:  0.9858871 recall:  0.978 f1:  0.98192763\n",
            "language:  lin precision:  0.9412998 recall:  0.898 f1:  0.91914016\n",
            "language:  min precision:  1.0 recall:  1.0 f1:  1.0\n",
            "language:  ksh precision:  0.95247936 recall:  0.922 f1:  0.9369919\n",
            "language:  pap precision:  0.9209979 recall:  0.886 f1:  0.90316\n",
            "language:  egl precision:  0.911157 recall:  0.882 f1:  0.8963413\n",
            "language:  bcl precision:  0.9655172 recall:  0.952 f1:  0.9587109\n",
            "language:  fra precision:  0.7690583 recall:  0.686 f1:  0.7251585\n",
            "language:  udm precision:  0.9519833 recall:  0.912 f1:  0.9315628\n",
            "language:  mya precision:  0.96245736 recall:  0.564 f1:  0.7112232\n",
            "language:  mai precision:  0.9190871 recall:  0.886 f1:  0.9022402\n",
            "language:  gle precision:  0.972973 recall:  0.936 f1:  0.9541284\n",
            "language:  tet precision:  0.9512195 recall:  0.936 f1:  0.9435484\n",
            "language:  kaa precision:  0.96465695 recall:  0.928 f1:  0.9459734\n",
            "language:  kor precision:  0.99589324 recall:  0.97 f1:  0.98277605\n",
            "language:  div precision:  0.9979508 recall:  0.974 f1:  0.9858299\n",
            "language:  lzh precision:  0.0 recall:  0.0 f1:  0.0\n",
            "language:  mzn precision:  0.9656566 recall:  0.956 f1:  0.9608039\n",
            "language:  dan precision:  0.9330544 recall:  0.892 f1:  0.9120654\n",
            "language:  nep precision:  0.8716356 recall:  0.842 f1:  0.8565614\n",
            "language:  bho precision:  0.96900827 recall:  0.938 f1:  0.95325196\n",
            "language:  zho precision:  0.6938776 recall:  0.068 f1:  0.12386155\n",
            "language:  tcy precision:  0.9773196 recall:  0.948 f1:  0.9624365\n",
            "language:  sah precision:  0.98125 recall:  0.942 f1:  0.9612244\n",
            "language:  che precision:  0.97373736 recall:  0.964 f1:  0.9688441\n",
            "language:  cym precision:  0.98582995 recall:  0.974 f1:  0.9798792\n",
            "language:  asm precision:  0.9775051 recall:  0.956 f1:  0.9666329\n",
            "language:  csb precision:  0.9450102 recall:  0.928 f1:  0.9364278\n",
            "language:  gag precision:  0.8803419 recall:  0.824 f1:  0.8512396\n",
            "language:  bar precision:  0.8896247 recall:  0.806 f1:  0.84575015\n",
            "language:  ang precision:  0.95878524 recall:  0.884 f1:  0.919875\n",
            "language:  arg precision:  0.9043659 recall:  0.87 f1:  0.8868501\n",
            "language:  por precision:  0.9538784 recall:  0.91 f1:  0.93142265\n",
            "language:  ckb precision:  0.98790324 recall:  0.98 f1:  0.9839357\n",
            "language:  vep precision:  0.95931476 recall:  0.896 f1:  0.92657703\n",
            "language:  chv precision:  0.96443516 recall:  0.922 f1:  0.9427402\n",
            "language:  sin precision:  0.97703546 recall:  0.936 f1:  0.9560776\n",
            "language:  nso precision:  0.9695122 recall:  0.954 f1:  0.9616935\n",
            "language:  mri precision:  0.9939516 recall:  0.986 f1:  0.9899598\n",
            "language:  sco precision:  0.8648069 recall:  0.806 f1:  0.83436847\n",
            "language:  chr precision:  0.997955 recall:  0.976 f1:  0.9868553\n",
            "language:  kab precision:  0.9189189 recall:  0.884 f1:  0.90112126\n",
            "language:  spa precision:  0.7626374 recall:  0.694 f1:  0.72670156\n",
            "language:  uig precision:  0.983368 recall:  0.946 f1:  0.96432203\n",
            "language:  jam precision:  0.9857434 recall:  0.968 f1:  0.976791\n",
            "language:  fas precision:  0.9087137 recall:  0.876 f1:  0.89205694\n",
            "language:  xmf precision:  0.9769392 recall:  0.932 f1:  0.9539406\n",
            "language:  frp precision:  0.8825996 recall:  0.842 f1:  0.86182183\n",
            "language:  ita precision:  0.8876652 recall:  0.806 f1:  0.84486365\n",
            "language:  mon precision:  0.9675456 recall:  0.954 f1:  0.960725\n",
            "language:  roh precision:  0.9389474 recall:  0.892 f1:  0.91487175\n",
            "language:  som precision:  0.94526315 recall:  0.898 f1:  0.9210256\n",
            "language:  glv precision:  0.98163265 recall:  0.962 f1:  0.9717172\n",
            "language:  myv precision:  0.9102296 recall:  0.872 f1:  0.89070463\n",
            "language:  ori precision:  0.9875776 recall:  0.954 f1:  0.9704984\n",
            "language:  koi precision:  0.8550107 recall:  0.802 f1:  0.8276573\n",
            "language:  nav precision:  0.997992 recall:  0.994 f1:  0.99599195\n",
            "language:  vec precision:  0.909465 recall:  0.884 f1:  0.89655167\n",
            "language:  rus precision:  0.88 recall:  0.792 f1:  0.83368415\n",
            "language:  azb precision:  0.9654471 recall:  0.95 f1:  0.9576612\n",
            "language:  grn precision:  0.9527721 recall:  0.928 f1:  0.9402228\n",
            "language:  fry precision:  0.94375 recall:  0.906 f1:  0.92448974\n",
            "language:  tuk precision:  0.96487606 recall:  0.934 f1:  0.949187\n",
            "language:  diq precision:  0.9298969 recall:  0.902 f1:  0.91573596\n",
            "language:  dsb precision:  0.943038 recall:  0.894 f1:  0.9178644\n",
            "language:  lij precision:  0.9203354 recall:  0.878 f1:  0.89866936\n",
            "language:  ton precision:  0.9979839 recall:  0.99 f1:  0.9939758\n",
            "language:  ara precision:  0.9202454 recall:  0.9 f1:  0.91001004\n",
            "language:  zh-yue precision:  0.1594203 recall:  0.022 f1:  0.0386643\n",
            "language:  jbo precision:  0.97987926 recall:  0.974 f1:  0.97693074\n",
            "language:  hye precision:  0.98571426 recall:  0.966 f1:  0.97575754\n",
            "language:  deu precision:  0.85714287 recall:  0.744 f1:  0.7965739\n",
            "language:  lat precision:  0.9483146 recall:  0.844 f1:  0.8931216\n",
            "language:  cdo precision:  0.99154335 recall:  0.938 f1:  0.9640287\n",
            "language:  mlg precision:  0.99198395 recall:  0.99 f1:  0.99099094\n",
            "language:  nno precision:  0.9466119 recall:  0.922 f1:  0.93414384\n",
            "language:  epo precision:  0.96666664 recall:  0.928 f1:  0.94693863\n",
            "language:  pol precision:  0.94335514 recall:  0.866 f1:  0.90302396\n",
            "language:  ben precision:  0.9762419 recall:  0.904 f1:  0.93873304\n",
            "language:  olo precision:  0.9471366 recall:  0.86 f1:  0.9014675\n",
            "language:  tel precision:  0.96443516 recall:  0.922 f1:  0.9427402\n",
            "language:  bxr precision:  0.96595746 recall:  0.908 f1:  0.9360825\n",
            "language:  mlt precision:  0.98380566 recall:  0.972 f1:  0.9778671\n",
            "language:  eng precision:  0.8603352 recall:  0.616 f1:  0.7179487\n",
            "language:  aym precision:  0.9539749 recall:  0.912 f1:  0.93251526\n",
            "language:  cor precision:  0.98582995 recall:  0.974 f1:  0.9798792\n",
            "language:  mrj precision:  0.9731405 recall:  0.942 f1:  0.957317\n",
            "language:  szl precision:  0.9701493 recall:  0.91 f1:  0.9391124\n",
            "language:  gla precision:  0.97946614 recall:  0.954 f1:  0.96656525\n",
            "language:  srn precision:  0.9691358 recall:  0.942 f1:  0.9553752\n",
            "language:  pnb precision:  0.9778672 recall:  0.972 f1:  0.9749247\n",
            "language:  glk precision:  0.9230769 recall:  0.888 f1:  0.90519875\n",
            "language:  lim precision:  0.9142857 recall:  0.896 f1:  0.90505046\n",
            "language:  swe precision:  0.9716599 recall:  0.96 f1:  0.9657947\n",
            "language:  slk precision:  0.91471213 recall:  0.858 f1:  0.8854489\n",
            "language:  san precision:  0.97330594 recall:  0.948 f1:  0.9604863\n",
            "language:  ukr precision:  0.92259413 recall:  0.882 f1:  0.90184045\n",
            "language:  heb precision:  0.9876289 recall:  0.958 f1:  0.9725888\n",
            "language:  kbd precision:  0.9769874 recall:  0.934 f1:  0.95501024\n",
            "language:  rup precision:  0.91792655 recall:  0.85 f1:  0.88265836\n",
            "language:  aze precision:  0.93670887 recall:  0.888 f1:  0.91170424\n",
            "language:  sqi precision:  0.96480334 recall:  0.932 f1:  0.9481179\n",
            "language:  ceb precision:  0.994 recall:  0.994 f1:  0.99399996\n",
            "language:  amh precision:  0.99593496 recall:  0.98 f1:  0.9879032\n",
            "language:  bos precision:  0.731441 recall:  0.67 f1:  0.6993736\n",
            "language:  tur precision:  0.8901099 recall:  0.81 f1:  0.8481674\n",
            "language:  wuu precision:  0.22580644 recall:  0.014 f1:  0.026365338\n",
            "language:  vie precision:  0.9919517 recall:  0.986 f1:  0.98896676\n",
            "language:  cat precision:  0.89224136 recall:  0.828 f1:  0.8589211\n",
            "language:  yid precision:  0.9858012 recall:  0.972 f1:  0.9788519\n",
            "language:  tgl precision:  0.98582995 recall:  0.974 f1:  0.9798792\n",
            "language:  cos precision:  0.9425051 recall:  0.918 f1:  0.93009114\n",
            "language:  tgk precision:  0.98155737 recall:  0.958 f1:  0.96963555\n",
            "language:  nds precision:  0.9649485 recall:  0.936 f1:  0.9502537\n",
            "language:  fao precision:  0.98178136 recall:  0.97 f1:  0.9758551\n",
            "language:  ibo precision:  0.92436975 recall:  0.88 f1:  0.90163934\n",
            "language:  ind precision:  0.8074534 recall:  0.78 f1:  0.7934892\n",
            "language:  ron precision:  0.9531568 recall:  0.936 f1:  0.9445004\n",
            "language:  orm precision:  0.9693251 recall:  0.948 f1:  0.9585439\n",
            "language:  fur precision:  0.9266247 recall:  0.884 f1:  0.9048106\n",
            "language:  scn precision:  0.95228213 recall:  0.918 f1:  0.9348268\n",
            "language:  ido precision:  0.9381663 recall:  0.88 f1:  0.90815276\n",
            "language:  que precision:  0.95634097 recall:  0.92 f1:  0.9378185\n",
            "language:  mwl precision:  0.93868923 recall:  0.888 f1:  0.9126413\n",
            "language:  nds-nl precision:  0.926078 recall:  0.902 f1:  0.9138804\n",
            "language:  kur precision:  0.96734697 recall:  0.948 f1:  0.95757574\n",
            "language:  isl precision:  0.9526749 recall:  0.926 f1:  0.939148\n",
            "language:  war precision:  0.995992 recall:  0.994 f1:  0.99499494\n",
            "language:  vol precision:  0.99383986 recall:  0.968 f1:  0.98074967\n",
            "language:  bpy precision:  0.98785424 recall:  0.976 f1:  0.9818913\n",
            "language:  mal precision:  0.9953917 recall:  0.864 f1:  0.92505354\n",
            "language:  sgs precision:  0.9791667 recall:  0.94 f1:  0.95918363\n",
            "language:  als precision:  0.91719747 recall:  0.864 f1:  0.8898043\n",
            "language:  ell precision:  0.9959514 recall:  0.984 f1:  0.98993963\n",
            "language:  hbs precision:  0.44393593 recall:  0.388 f1:  0.41408744\n",
            "language:  uzb precision:  0.9695122 recall:  0.954 f1:  0.9616935\n",
            "language:  afr precision:  0.98178136 recall:  0.97 f1:  0.9758551\n",
            "language:  mkd precision:  0.9876289 recall:  0.958 f1:  0.9725888\n",
            "language:  hrv precision:  0.55125284 recall:  0.484 f1:  0.5154419\n",
            "language:  vls precision:  0.8210526 recall:  0.78 f1:  0.79999995\n",
            "language:  lit precision:  0.9623894 recall:  0.87 f1:  0.9138655\n",
            "language:  nci precision:  0.84565216 recall:  0.778 f1:  0.8104166\n",
            "language:  kir precision:  0.9650924 recall:  0.94 f1:  0.95238084\n",
            "language:  mdf precision:  0.95454544 recall:  0.924 f1:  0.9390243\n",
            "language:  hif precision:  0.94255316 recall:  0.886 f1:  0.91340196\n",
            "language:  nld precision:  0.9253112 recall:  0.892 f1:  0.9083502\n",
            "language:  new precision:  0.97183096 recall:  0.966 f1:  0.96890664\n",
            "language:  hin precision:  0.9877551 recall:  0.968 f1:  0.9777777\n",
            "language:  tat precision:  0.9410526 recall:  0.894 f1:  0.916923\n",
            "language:  hat precision:  0.98163265 recall:  0.962 f1:  0.9717172\n",
            "language:  jpn precision:  0.7761194 recall:  0.104 f1:  0.1834215\n",
            "language:  ava precision:  0.8382688 recall:  0.736 f1:  0.7838125\n",
            "language:  tyv precision:  0.98350513 recall:  0.954 f1:  0.96852785\n",
            "language:  ina precision:  0.9426752 recall:  0.888 f1:  0.9145211\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-jFoYvQVzg0C",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "raw_metrics = pd.DataFrame(\n",
        "    {'precision': ps,\n",
        "     'recall': rs,\n",
        "     'f1': fs\n",
        "    })"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vXV6tZ4Dzh4L",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "metrics = raw_metrics.rename(class_to_language, axis='index')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3DIQwh8Z0Gq8",
        "colab_type": "code",
        "outputId": "e930a499-4a48-46f7-fe36-39024e27791d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 118
        }
      },
      "source": [
        "print(metrics.head())"
      ],
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "     precision  recall        f1\n",
            "pcd   0.778302   0.660  0.714286\n",
            "sme   0.949045   0.894  0.920700\n",
            "mhr   0.939583   0.902  0.920408\n",
            "bak   0.963489   0.950  0.956697\n",
            "est   0.952915   0.850  0.898520\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Tn_XJXIs45JJ",
        "colab_type": "code",
        "outputId": "89a5df89-5034-4742-fcde-c2a0c1fb772a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 402
        }
      },
      "source": [
        "bad_quality_metrics = metrics[metrics.f1 < 0.7]\n",
        "bad_quality_metrics"
      ],
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>precision</th>\n",
              "      <th>recall</th>\n",
              "      <th>f1</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>khm</th>\n",
              "      <td>0.862500</td>\n",
              "      <td>0.414</td>\n",
              "      <td>0.559459</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>be-tarask</th>\n",
              "      <td>0.686722</td>\n",
              "      <td>0.662</td>\n",
              "      <td>0.674134</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>bod</th>\n",
              "      <td>0.943820</td>\n",
              "      <td>0.168</td>\n",
              "      <td>0.285229</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>tha</th>\n",
              "      <td>0.963899</td>\n",
              "      <td>0.534</td>\n",
              "      <td>0.687259</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>lzh</th>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>zho</th>\n",
              "      <td>0.693878</td>\n",
              "      <td>0.068</td>\n",
              "      <td>0.123862</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>zh-yue</th>\n",
              "      <td>0.159420</td>\n",
              "      <td>0.022</td>\n",
              "      <td>0.038664</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>bos</th>\n",
              "      <td>0.731441</td>\n",
              "      <td>0.670</td>\n",
              "      <td>0.699374</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>wuu</th>\n",
              "      <td>0.225806</td>\n",
              "      <td>0.014</td>\n",
              "      <td>0.026365</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>hbs</th>\n",
              "      <td>0.443936</td>\n",
              "      <td>0.388</td>\n",
              "      <td>0.414087</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>hrv</th>\n",
              "      <td>0.551253</td>\n",
              "      <td>0.484</td>\n",
              "      <td>0.515442</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>jpn</th>\n",
              "      <td>0.776119</td>\n",
              "      <td>0.104</td>\n",
              "      <td>0.183421</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "           precision  recall        f1\n",
              "khm         0.862500   0.414  0.559459\n",
              "be-tarask   0.686722   0.662  0.674134\n",
              "bod         0.943820   0.168  0.285229\n",
              "tha         0.963899   0.534  0.687259\n",
              "lzh         0.000000   0.000  0.000000\n",
              "zho         0.693878   0.068  0.123862\n",
              "zh-yue      0.159420   0.022  0.038664\n",
              "bos         0.731441   0.670  0.699374\n",
              "wuu         0.225806   0.014  0.026365\n",
              "hbs         0.443936   0.388  0.414087\n",
              "hrv         0.551253   0.484  0.515442\n",
              "jpn         0.776119   0.104  0.183421"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 57
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wqs-mCe43x4e",
        "colab_type": "code",
        "outputId": "b67d48fa-25fc-4790-d77b-a795d8b3227f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 402
        }
      },
      "source": [
        "bad_quality_langs = list(bad_quality_metrics.index)\n",
        "bad_quality_labels = labels.loc[labels['Label'].isin(bad_quality_langs)]\n",
        "bad_quality_labels"
      ],
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Label</th>\n",
              "      <th>English</th>\n",
              "      <th>Wiki Code</th>\n",
              "      <th>ISO 369-3</th>\n",
              "      <th>Language family</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>17</th>\n",
              "      <td>be-tarask</td>\n",
              "      <td>Belarusian (Taraschkewiza)</td>\n",
              "      <td>be-tarask</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Indo-European</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>22</th>\n",
              "      <td>bod</td>\n",
              "      <td>Tibetan</td>\n",
              "      <td>bo</td>\n",
              "      <td>bod</td>\n",
              "      <td>Sino-Tibetan</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>23</th>\n",
              "      <td>bos</td>\n",
              "      <td>Bosnian</td>\n",
              "      <td>bs</td>\n",
              "      <td>bos</td>\n",
              "      <td>Indo-European</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>73</th>\n",
              "      <td>hbs</td>\n",
              "      <td>Serbo-Croatian</td>\n",
              "      <td>sh</td>\n",
              "      <td>hbs</td>\n",
              "      <td>Indo-European</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>77</th>\n",
              "      <td>hrv</td>\n",
              "      <td>Croatian</td>\n",
              "      <td>hr</td>\n",
              "      <td>hrv</td>\n",
              "      <td>Indo-European</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>92</th>\n",
              "      <td>jpn</td>\n",
              "      <td>Japanese</td>\n",
              "      <td>ja</td>\n",
              "      <td>jpn</td>\n",
              "      <td>Japonic</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>99</th>\n",
              "      <td>khm</td>\n",
              "      <td>Central Khmer</td>\n",
              "      <td>km</td>\n",
              "      <td>khm</td>\n",
              "      <td>Austronesian</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>123</th>\n",
              "      <td>lzh</td>\n",
              "      <td>Literary Chinese</td>\n",
              "      <td>zh-classical</td>\n",
              "      <td>lzh</td>\n",
              "      <td>Sino-Tibetan</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>207</th>\n",
              "      <td>tha</td>\n",
              "      <td>Thai</td>\n",
              "      <td>th</td>\n",
              "      <td>tha</td>\n",
              "      <td>Tai-Kadai</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>227</th>\n",
              "      <td>wuu</td>\n",
              "      <td>Wu Chinese</td>\n",
              "      <td>wuu</td>\n",
              "      <td>wuu</td>\n",
              "      <td>Sino-Tibetan</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>233</th>\n",
              "      <td>zh-yue</td>\n",
              "      <td>Cantonese</td>\n",
              "      <td>zh-yue</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Sino-Tibetan</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>234</th>\n",
              "      <td>zho</td>\n",
              "      <td>Standard Chinese</td>\n",
              "      <td>zh</td>\n",
              "      <td>zho</td>\n",
              "      <td>Sino-Tibetan</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "         Label                     English  ... ISO 369-3 Language family\n",
              "17   be-tarask  Belarusian (Taraschkewiza)  ...       NaN   Indo-European\n",
              "22         bod                     Tibetan  ...       bod    Sino-Tibetan\n",
              "23         bos                     Bosnian  ...       bos   Indo-European\n",
              "73         hbs              Serbo-Croatian  ...       hbs   Indo-European\n",
              "77         hrv                    Croatian  ...       hrv   Indo-European\n",
              "92         jpn                    Japanese  ...       jpn         Japonic\n",
              "99         khm               Central Khmer  ...       khm    Austronesian\n",
              "123        lzh            Literary Chinese  ...       lzh    Sino-Tibetan\n",
              "207        tha                        Thai  ...       tha       Tai-Kadai\n",
              "227        wuu                  Wu Chinese  ...       wuu    Sino-Tibetan\n",
              "233     zh-yue                   Cantonese  ...       NaN    Sino-Tibetan\n",
              "234        zho            Standard Chinese  ...       zho    Sino-Tibetan\n",
              "\n",
              "[12 rows x 5 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 58
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gUkII9aS9iK1",
        "colab_type": "code",
        "outputId": "4c2bf3e3-915c-4cd7-bb1c-584d464d0d7e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 487
        }
      },
      "source": [
        "print(x_train[x_train.sc == language_to_class['wuu']])\n",
        "print(x_train[x_train.sc == language_to_class['lzh']])"
      ],
      "execution_count": 66,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "                                                     sent   sc\n",
            "19      UNC有得一只历史悠久个'诚信守则'。渠是由学堂个诚信法庭（Honor Court）来执行个...  187\n",
            "276     武器，是人类为达到杀伤或者防御个目的制造个器械。武器从人类文明发展开始就有出现，之后，伴随战...  187\n",
            "387     弗过，太后个信任是一方面，具体西班牙个政治局面又是另外一番情形：贵族对迭个外国人一点也无信任...  187\n",
            "720     从12世纪挨末阶段开始，日本个统治权就转移到日本武士贵族个手里向。到13世纪，出身清和源氏个...  187\n",
            "930     箇种毛病潜伏期一般勒7日天以内，病人一般表现为流感箇浪个症状，像发寒热，咳嗽，少痰，有种辰光...  187\n",
            "...                                                   ...  ...\n",
            "116805  Linux個低成本、強大個定制功能搭良好個移植性能，使得Linux來嵌入式系統方面也得到廣泛...  187\n",
            "116947  余杭區勒拉杭嘉湖平原南端，西依天目山，南瀕錢塘江，是長江三角洲个圓心地。地理座標爲北緯30°...  187\n",
            "117034  北師大是以京師大學堂師範專業做基礎，搭仔由北京個高校匯聚一批勒師範教育領域窮有聲望個名師組建...  187\n",
            "117037  到清中期（約18世紀），傳奇開始衰微，向書齋文學轉化，彈詞卻逐步興盛起來了。出版之錢德蒼編个...  187\n",
            "117257  摩嘉娜夺权计划失败后，带领奄奄一息个莫高絲逃出卡美洛特，徕萨温节夜里，摩嘉娜徕莫高絲个要求之...  187\n",
            "\n",
            "[500 rows x 2 columns]\n",
            "                                                     sent  sc\n",
            "336     武漢市，亦稱以漢，乃中華鄂省之會，亦為七大都市於中華之中原也。方八千四百六十七公里，於西元二...  97\n",
            "420     按黃帝為法，數有十等。及其用也，乃有三焉。十等者，謂「億、兆、京、垓、秭、壤、溝、澗、正、載...  97\n",
            "1254    范蠡浮海出齊，變姓名，自謂鴟夷子皮，耕於海畔，苦身戮力，父子治產。居無幾何，致產數千萬。齊人...  97\n",
            "1414    周迪據臨川反，詔昭達便道征之。迪敗走，徵為護軍將軍，改封邵武縣侯。四年，陳寶應納迪，共寇臨川...  97\n",
            "1549    喇克達長子敬德，康熙十一年(一六七二年)封三等奉恩將軍，四十七年(一七零八年)卒。敬德次子班...  97\n",
            "...                                                   ...  ..\n",
            "116047  士族者，或曰門第、衣冠、世族、門閥、勢族、世家、巨室。蓋謂世居要位之高門也，世族所居不同，中...  97\n",
            "116419  淳化二年秋七月，李繼遷請降，以爲銀州觀察使，賜姓名趙保吉。繼捧至夏州數月，即言繼遷悔過歸款，...  97\n",
            "116596  孟嘗君怨秦，將以齊為韓、魏攻楚，因與韓、魏攻秦，而借兵食於西周。蘇代為西周謂曰：「君以齊為韓...  97\n",
            "116749  陳敏之亂，弘以侃為江夏太守，加鷹揚將軍。侃備威儀，迎母官舍，鄉裡榮之。敏遣其弟恢來寇武昌，侃...  97\n",
            "117497  同年，太后崩。絳侯周勃、陳平諸臣共謀誅呂。朱虛侯章已殺呂產，文帝使人持節勞章。朱虛侯欲奪節信...  97\n",
            "\n",
            "[500 rows x 2 columns]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dH862g-VYj8J",
        "colab_type": "text"
      },
      "source": [
        "# Fix Chinese langs"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yltabFr0UYF_",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "34d578ab-68f4-4cd7-cc38-d08d162c7838"
      },
      "source": [
        "# Extract Unigrams\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "unigramVectorizer = CountVectorizer(analyzer='char', ngram_range=(1,1))\n",
        "X_unigram_train_raw = unigramVectorizer.fit_transform(x_train)\n",
        "X_unigram_test_raw = unigramVectorizer.transform(x_test)\n",
        "\n",
        "\n",
        "unigramFeatures = unigramVectorizer.get_feature_names()\n",
        "\n",
        "print('Number of unigrams in training set:', len(unigramFeatures))"
      ],
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of unigrams in training set: 4\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KXhcTGMaYZXt",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 346
        },
        "outputId": "f449f432-dd28-40df-fdf4-863b20e062f0"
      },
      "source": [
        "def train_lang_dict(X_raw_counts, y_train):\n",
        "    lang_dict = {}\n",
        "    for i in range(len(y_train)):\n",
        "        lang = y_train[i]\n",
        "        v = np.array(X_raw_counts[i])\n",
        "        if not lang in lang_dict:\n",
        "            lang_dict[lang] = v\n",
        "        else:\n",
        "            lang_dict[lang] += v\n",
        "            \n",
        "    # to relative\n",
        "    for lang in lang_dict:\n",
        "        v = lang_dict[lang]\n",
        "        lang_dict[lang] = v / np.sum(v)\n",
        "        \n",
        "    return lang_dict\n",
        "\n",
        "language_dict_unigram = train_lang_dict(X_unigram_train_raw.toarray(), y_train.values)\n",
        "\n",
        "# Collect relevant chars per language\n",
        "def getRelevantCharsPerLanguage(features, language_dict, significance=1e-5):\n",
        "    relevantCharsPerLanguage = {}\n",
        "    for lang in languages:\n",
        "        chars = []\n",
        "        relevantCharsPerLanguage[lang] = chars\n",
        "        v = language_dict[lang]\n",
        "        for i in range(len(v)):\n",
        "            if v[i] > significance:\n",
        "                chars.append(features[i])\n",
        "    return relevantCharsPerLanguage\n",
        "\n",
        "relevantCharsPerLanguage = getRelevantCharsPerLanguage(unigramFeatures, language_dict_unigram)\n",
        "    \n",
        "# Print number of unigrams per language\n",
        "for lang in languages:    \n",
        "    print(lang, len(relevantCharsPerLanguage[lang]))"
      ],
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-61-592510daad2e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mlang_dict\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m \u001b[0mlanguage_dict_unigram\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_lang_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_unigram_train_raw\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;31m# Collect relevant chars per language\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-61-592510daad2e>\u001b[0m in \u001b[0;36mtrain_lang_dict\u001b[0;34m(X_raw_counts, y_train)\u001b[0m\n\u001b[1;32m      4\u001b[0m         \u001b[0mlang\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[0mv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_raw_counts\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mlang\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mlang_dict\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m             \u001b[0mlang_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlang\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mv\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: unhashable type: 'numpy.ndarray'"
          ]
        }
      ]
    }
  ]
}