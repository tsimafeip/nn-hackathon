{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Copy of hackaton.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "kpmSbtrCklRB",
        "colab_type": "code",
        "outputId": "37b6dcae-3918-4165-bb3a-ff951c6f1a71",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lNLhWWCxYfLf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import re\n",
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import f1_score\n",
        "import scipy\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import logging\n",
        "import multiprocessing\n",
        "import gensim\n",
        "from gensim.models import Word2Vec\n",
        "from sklearn.decomposition import PCA"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z8IzA7Rx-VzU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.layers import Dense, Input, LSTM, Embedding, Dropout, Activation, Conv1D\n",
        "from tensorflow.keras import Sequential\n",
        "from tensorflow.keras.layers import Bidirectional, GlobalMaxPooling1D, GlobalMaxPool1D, MaxPooling1D, Flatten, concatenate\n",
        "from tensorflow.keras.models import Model, load_model\n",
        "from tensorflow.keras import initializers, regularizers, constraints, optimizers, layers\n",
        "from tensorflow.keras.callbacks import CSVLogger, ModelCheckpoint, EarlyStopping\n",
        "\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from tensorflow.keras import backend as K"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H1vZGIRYYR9G",
        "colab_type": "text"
      },
      "source": [
        "# Hanna's data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uFKrCtLZYBcV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import zipfile\n",
        "with zipfile.ZipFile('./drive/My Drive/hackaton/wili-2018.zip', 'r') as zip_ref:\n",
        "    zip_ref.extractall('./')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZcwIDfWAYOUl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "data_path = '/content/drive/My Drive/Colab Notebooks/Hackathon/'\n",
        "wili_data_path = ''\n",
        "\n",
        "x_train_path = wili_data_path + 'x_train.txt'\n",
        "x_test_path = wili_data_path + 'x_test.txt'\n",
        "y_train_path = wili_data_path + 'y_train.txt'\n",
        "y_test_path = wili_data_path + 'y_test.txt'\n",
        "labels_path = wili_data_path + 'labels.csv'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Oc94smEhjbA_",
        "colab_type": "text"
      },
      "source": [
        "# Download data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "75potnAshWU4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "data_path = '/content/drive/My Drive/Colab Notebooks/Hackathon/'\n",
        "wili_data_path = data_path + 'wili-2018/'\n",
        "\n",
        "x_train_path = wili_data_path + 'x_train.txt'\n",
        "x_test_path = wili_data_path + 'x_test.txt'\n",
        "y_train_path = wili_data_path + 'y_train.txt'\n",
        "y_test_path = wili_data_path + 'y_test.txt'\n",
        "labels_path = wili_data_path + 'labels.csv'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NhsiujWS3QR8",
        "colab_type": "code",
        "outputId": "1940d872-54c6-41f8-f65a-bbe8645f505e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 419
        }
      },
      "source": [
        "labels = pd.read_csv(labels_path, sep=';')\n",
        "labels = labels.drop(labels=['German', 'Writing system', 'Remarks', 'Synonyms'], axis=1)\n",
        "labels"
      ],
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Label</th>\n",
              "      <th>English</th>\n",
              "      <th>Wiki Code</th>\n",
              "      <th>ISO 369-3</th>\n",
              "      <th>Language family</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>ace</td>\n",
              "      <td>Achinese</td>\n",
              "      <td>ace</td>\n",
              "      <td>ace</td>\n",
              "      <td>Austronesian</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>afr</td>\n",
              "      <td>Afrikaans</td>\n",
              "      <td>af</td>\n",
              "      <td>afr</td>\n",
              "      <td>Indo-European</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>als</td>\n",
              "      <td>Alemannic German</td>\n",
              "      <td>als</td>\n",
              "      <td>gsw</td>\n",
              "      <td>Indo-European</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>amh</td>\n",
              "      <td>Amharic</td>\n",
              "      <td>am</td>\n",
              "      <td>amh</td>\n",
              "      <td>Afro-Asiatic</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>ang</td>\n",
              "      <td>Old English</td>\n",
              "      <td>ang</td>\n",
              "      <td>ang</td>\n",
              "      <td>Indo-European</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>230</th>\n",
              "      <td>yid</td>\n",
              "      <td>Yiddish</td>\n",
              "      <td>yi</td>\n",
              "      <td>yid</td>\n",
              "      <td>Indo-European</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>231</th>\n",
              "      <td>yor</td>\n",
              "      <td>Yoruba</td>\n",
              "      <td>yo</td>\n",
              "      <td>yor</td>\n",
              "      <td>Niger-Congo</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>232</th>\n",
              "      <td>zea</td>\n",
              "      <td>Zeeuws</td>\n",
              "      <td>zea</td>\n",
              "      <td>zea</td>\n",
              "      <td>Indo-European</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>233</th>\n",
              "      <td>zh-yue</td>\n",
              "      <td>Cantonese</td>\n",
              "      <td>zh-yue</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Sino-Tibetan</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>234</th>\n",
              "      <td>zho</td>\n",
              "      <td>Standard Chinese</td>\n",
              "      <td>zh</td>\n",
              "      <td>zho</td>\n",
              "      <td>Sino-Tibetan</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>235 rows × 5 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "      Label           English Wiki Code ISO 369-3 Language family\n",
              "0       ace          Achinese       ace       ace    Austronesian\n",
              "1       afr         Afrikaans        af       afr   Indo-European\n",
              "2       als  Alemannic German       als       gsw   Indo-European\n",
              "3       amh           Amharic        am       amh    Afro-Asiatic\n",
              "4       ang      Old English        ang       ang   Indo-European\n",
              "..      ...               ...       ...       ...             ...\n",
              "230     yid           Yiddish        yi       yid   Indo-European\n",
              "231     yor            Yoruba        yo       yor     Niger-Congo\n",
              "232     zea            Zeeuws       zea       zea   Indo-European\n",
              "233  zh-yue         Cantonese    zh-yue       NaN    Sino-Tibetan\n",
              "234     zho  Standard Chinese        zh       zho    Sino-Tibetan\n",
              "\n",
              "[235 rows x 5 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 45
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FpH6qYnlYrBN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class_to_language = labels.Label.astype(str).to_dict()\n",
        "language_to_class = {v: k for k, v in class_to_language.items()}\n",
        "num_classes = len(class_to_language)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UDFr3-rKmDdv",
        "colab_type": "code",
        "outputId": "938ed293-402a-4627-9dbc-641bfc43b88d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "print(list(language_to_class.items())[:5])\n",
        "num_classes"
      ],
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[('ace', 0), ('afr', 1), ('als', 2), ('amh', 3), ('ang', 4)]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "235"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 50
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YBqkjck1Yc7q",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def read_data():\n",
        "    with open(x_train_path) as f:\n",
        "        mylist = f.read().splitlines() \n",
        "    x_train = pd.DataFrame(mylist, columns=[\"sent\"])\n",
        "    with open(x_test_path) as f:\n",
        "        mylist = f.read().splitlines() \n",
        "    x_test = pd.DataFrame(mylist, columns=[\"sent\"])\n",
        "\n",
        "    with open(y_train_path) as f:\n",
        "        mylist = f.read().splitlines() \n",
        "    y_train = []\n",
        "    for lan in mylist:\n",
        "        y_train.append(language_to_class[lan])\n",
        "    y_train = pd.DataFrame(y_train)\n",
        "    with open(y_test_path) as f:\n",
        "        mylist = f.read().splitlines()\n",
        "\n",
        "    y_test = []\n",
        "    for lan in mylist:\n",
        "        y_test.append(language_to_class[lan])\n",
        "    y_test = pd.DataFrame(y_test)\n",
        "    \n",
        "    return (x_train, y_train), (x_test, y_test)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lRo0Wf1lZXsE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "(x_train, y_train), (x_test, y_test) = read_data()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zhKRW6V8hhIc",
        "colab_type": "code",
        "outputId": "e1b32c95-7493-4485-8075-52c900226579",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 255
        }
      },
      "source": [
        "print(x_train.head())\n",
        "print(y_train.head())\n",
        "\n",
        "print(x_train.shape)\n",
        "print(x_test.shape)"
      ],
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "                                                sent\n",
            "0  Klement Gottwaldi surnukeha palsameeriti ning ...\n",
            "1  Sebes, Joseph; Pereira Thomas (1961) (på eng)....\n",
            "2  भारतीय स्वातन्त्र्य आन्दोलन राष्ट्रीय एवम क्षे...\n",
            "3  Après lo cort periòde d'establiment a Basilèa,...\n",
            "4  ถนนเจริญกรุง (อักษรโรมัน: Thanon Charoen Krung...\n",
            "     0\n",
            "0   52\n",
            "1  198\n",
            "2  124\n",
            "3  155\n",
            "4  207\n",
            "(117500, 1)\n",
            "(117500, 1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9zcUciRigFYu",
        "colab_type": "code",
        "outputId": "b71ef601-0fff-4919-bfcd-6099b1acd2b0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 265
        }
      },
      "source": [
        "lens = x_train.sent.str.split(' ').str.len().values\n",
        "plt.hist(lens, bins=np.linspace(0,1000,20), facecolor='blue', alpha=0.9)\n",
        "plt.show()"
      ],
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4xdCSiHnhlX1",
        "colab_type": "text"
      },
      "source": [
        "# Experiment on small dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sFTqOQmChknq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# small_dataset = pd.read_csv(data_path + 'kaggle_dataset/dataset.csv')\n",
        "# small_dataset"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LwtWy38ehtGO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# X=small_dataset['Text']\n",
        "# y=small_dataset['language']\n",
        "\n",
        "# X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# print(len(X_train))\n",
        "# print(len(X_test))\n",
        "# print(len(y_train))\n",
        "# print(len(y_test))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XNoYz_IwjeY_",
        "colab_type": "text"
      },
      "source": [
        "# Embedding **data**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bEr8pklSjQLP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "filter_out = '!\"#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n1234567890'\n",
        "tokenizer = Tokenizer(filters=filter_out, lower=True)\n",
        "tokenizer.fit_on_texts(x_train.sent)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a5jxVwDcftcn",
        "colab_type": "text"
      },
      "source": [
        "Выбираю какое количество слов оставить. Если оставлять все, то будет 1500000, что слишком много. Можно поменять `count` и оно покажет сколько слов встречается больше, чем `count` раз"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4xKQt34TAG2-",
        "colab_type": "code",
        "outputId": "37e81b02-fc27-4315-a2e8-67116169bb71",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# try different values of `count`\n",
        "# show how many words are presenting more than `count` times\n",
        "count = 10\n",
        "frequent_words = [w for w,c in tokenizer.word_counts.items() if c > count]\n",
        "len(frequent_words)"
      ],
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "64157"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 57
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QJacuBHhHEng",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "embed_size = 300 # how big is each word vector\n",
        "max_features = len(frequent_words) # how many unique words to use (i.e num rows in embedding vector)\n",
        "maxlen = 200 # max number of words in a question to use\n",
        "\n",
        "## Tokenize the sentences\n",
        "tokenizer = Tokenizer(num_words=max_features, filters=filter_out, lower=True)\n",
        "\n",
        "def prepare_data():\n",
        "    x_tr, x_val, y_tr, y_val = train_test_split(x_train, y_train, test_size=0.1, random_state=1, stratify=y_train.values)\n",
        "   \n",
        "    tokenizer.fit_on_texts(x_tr.sent)\n",
        "\n",
        "    train_X = tokenizer.texts_to_sequences(x_tr.sent)\n",
        "    val_X = tokenizer.texts_to_sequences(x_val.sent)\n",
        "\n",
        "    ## Pad the sentences \n",
        "    train_X = pad_sequences(train_X, maxlen=maxlen)\n",
        "    val_X = pad_sequences(val_X, maxlen=maxlen)\n",
        "\n",
        "    return (train_X, y_tr), (val_X, y_val)\n",
        "    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qW1oK9wfHEnm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "(train_X, train_Y), (val_X, val_Y) = prepare_data()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HxyyqzKsgF9K",
        "colab_type": "text"
      },
      "source": [
        "Проверила, что в тренировочном и валидационном датасетах встречаются все классы. Балансировка регулируется вот этим параметром: stratify=y_train.values"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R1iJF7cyU5iK",
        "colab_type": "code",
        "outputId": "a6226ab8-1cf0-408a-c71b-791561ecd8d2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "len(set(val_Y.values.reshape(1, -1)[0]))"
      ],
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "235"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 60
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aPITYtR8UqNx",
        "colab_type": "code",
        "outputId": "86d6536d-b329-4353-cd3b-4f8b2a7f2bb2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "len(set(train_Y.values.reshape(1, -1)[0]))"
      ],
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "235"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 61
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TYuurOSQgYUl",
        "colab_type": "text"
      },
      "source": [
        "Ниже меняю представление векторов ответов из [1, 23, 10,...] в вектор длины 235 и с 1 на месте соответсвующего языка"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b0iIyf4bIXg_",
        "colab_type": "code",
        "outputId": "7a024daa-7410-4b6a-c9fd-b9a6889c5eb6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 105
        }
      },
      "source": [
        "lb = LabelEncoder()\n",
        "y = lb.fit_transform(train_Y.values)\n",
        "dummy_y_train = to_categorical(y)\n",
        "print(len(dummy_y_train))\n",
        "print(len(dummy_y_train[0]))"
      ],
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "105750\n",
            "235\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/preprocessing/_label.py:251: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
            "  y = column_or_1d(y, warn=True)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vCBYAruyIxUm",
        "colab_type": "code",
        "outputId": "6fc3ebdb-87cf-4d92-a8e8-78d210420efb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        }
      },
      "source": [
        "y = lb.fit_transform(val_Y.values)\n",
        "dummy_y_val = to_categorical(y)"
      ],
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/preprocessing/_label.py:251: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
            "  y = column_or_1d(y, warn=True)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MeXXgZJ6miyl",
        "colab_type": "text"
      },
      "source": [
        "# Metrics"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vp4rfHfplm6O",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def f1(y_true, y_pred): #taken from old keras source code\n",
        "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
        "    possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
        "    predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
        "    precision = true_positives / (predicted_positives + K.epsilon())\n",
        "    recall = true_positives / (possible_positives + K.epsilon())\n",
        "    f1_val = 2*(precision*recall)/(precision+recall+K.epsilon())\n",
        "    return f1_val"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Dzi3zyNsmNIR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def recall(y_true, y_pred): #taken from old keras source code\n",
        "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
        "    possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
        "    predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
        "    return true_positives / (possible_positives + K.epsilon())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JX13CaozmRhy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def precision(y_true, y_pred): #taken from old keras source code\n",
        "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
        "    possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
        "    predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
        "    return true_positives / (predicted_positives + K.epsilon())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VtGH8P89gTXf",
        "colab_type": "text"
      },
      "source": [
        "#CNN"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vdsE9d9SjdsW",
        "colab_type": "code",
        "outputId": "6f7e28a6-edf2-403c-f7d9-9abaf8d0ceb2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 493
        }
      },
      "source": [
        "cnn_model = Sequential()\n",
        "cnn_model.add(Embedding(max_features, embed_size,input_shape=(maxlen,)))\n",
        "cnn_model.add(Conv1D(256, 5, activation='relu', input_shape=(embed_size,)))\n",
        "cnn_model.add(MaxPooling1D(5))\n",
        "cnn_model.add(Dropout(0.2))\n",
        "cnn_model.add(Conv1D(256, 5, activation='relu'))\n",
        "cnn_model.add(MaxPooling1D(5))\n",
        "cnn_model.add(Flatten())\n",
        "cnn_model.add(Dense(512, activation=\"relu\"))\n",
        "cnn_model.add(Dropout(0.2))\n",
        "cnn_model.add(Dense(num_classes, activation='softmax'))\n",
        "cnn_model.summary()"
      ],
      "execution_count": 68,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_1 (Embedding)      (None, 200, 300)          19247100  \n",
            "_________________________________________________________________\n",
            "conv1d_2 (Conv1D)            (None, 196, 256)          384256    \n",
            "_________________________________________________________________\n",
            "max_pooling1d_2 (MaxPooling1 (None, 39, 256)           0         \n",
            "_________________________________________________________________\n",
            "dropout_2 (Dropout)          (None, 39, 256)           0         \n",
            "_________________________________________________________________\n",
            "conv1d_3 (Conv1D)            (None, 35, 256)           327936    \n",
            "_________________________________________________________________\n",
            "max_pooling1d_3 (MaxPooling1 (None, 7, 256)            0         \n",
            "_________________________________________________________________\n",
            "flatten_1 (Flatten)          (None, 1792)              0         \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 512)               918016    \n",
            "_________________________________________________________________\n",
            "dropout_3 (Dropout)          (None, 512)               0         \n",
            "_________________________________________________________________\n",
            "dense_2 (Dense)              (None, 235)               120555    \n",
            "=================================================================\n",
            "Total params: 20,997,863\n",
            "Trainable params: 20,997,863\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Sndko3XlZ806",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "cnn_model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy', f1, recall, precision])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Aa9rf4_Oq6nX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "cnn_cp_path = data_path+'model_cnn.hdf5'\n",
        "cnn_cp=ModelCheckpoint(cnn_cp_path, monitor='val_f1',save_best_only=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LIhz76oFHo_c",
        "colab_type": "code",
        "outputId": "9e67ff39-9937-4548-d3fb-6b2a780b8f86",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 392
        }
      },
      "source": [
        "cnn_model.fit(train_X, dummy_y_train, batch_size=512, epochs=5, \n",
        "              validation_data=(val_X, dummy_y_val),\n",
        "              callbacks = [cnn_cp]\n",
        "             )"
      ],
      "execution_count": 104,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/5\n",
            " 51/207 [======>.......................] - ETA: 1:01 - loss: 0.1227 - accuracy: 0.9635 - pres: 0.8565 - rec: 0.8453 - met: 0.8480 - recall: 0.9564 - precision: 0.9917"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-104-2c58e4b68001>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m cnn_model.fit(train_X, dummy_y_train, batch_size=512, epochs=5, \n\u001b[0;32m----> 2\u001b[0;31m               \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval_X\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdummy_y_val\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m           \u001b[0;31m#    callbacks = [cnn_cp]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m              )\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36m_method_wrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     64\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_method_wrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_in_multi_worker_mode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 66\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     67\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m     \u001b[0;31m# Running inside `run_distribute_coordinator` already.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m    846\u001b[0m                 batch_size=batch_size):\n\u001b[1;32m    847\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 848\u001b[0;31m               \u001b[0mtmp_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    849\u001b[0m               \u001b[0;31m# Catch OutOfRangeError for Datasets of unknown size.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    850\u001b[0m               \u001b[0;31m# This blocks until the batch has finished executing.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    578\u001b[0m         \u001b[0mxla_context\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mExit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    579\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 580\u001b[0;31m       \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    581\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    582\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mtracing_count\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    609\u001b[0m       \u001b[0;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    610\u001b[0m       \u001b[0;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 611\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=not-callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    612\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    613\u001b[0m       \u001b[0;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2418\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2419\u001b[0m       \u001b[0mgraph_function\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_define_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2420\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_filtered_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2421\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2422\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_filtered_call\u001b[0;34m(self, args, kwargs)\u001b[0m\n\u001b[1;32m   1663\u001b[0m          if isinstance(t, (ops.Tensor,\n\u001b[1;32m   1664\u001b[0m                            resource_variable_ops.BaseResourceVariable))),\n\u001b[0;32m-> 1665\u001b[0;31m         self.captured_inputs)\n\u001b[0m\u001b[1;32m   1666\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1667\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_call_flat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcancellation_manager\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1744\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1745\u001b[0m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0;32m-> 1746\u001b[0;31m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0m\u001b[1;32m   1747\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[1;32m   1748\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    596\u001b[0m               \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    597\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattrs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 598\u001b[0;31m               ctx=ctx)\n\u001b[0m\u001b[1;32m    599\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    600\u001b[0m           outputs = execute.execute_with_cancellation(\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0;32m---> 60\u001b[0;31m                                         inputs, attrs, num_outputs)\n\u001b[0m\u001b[1;32m     61\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AkmpFZmyUjF8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "dependencies = {\n",
        "     'f1': f1,\n",
        "     'recall': recall,\n",
        "     'precision': precision\n",
        "}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qji8xqW2WiS-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# cnn_model = load_model(cnn_cp_path, custom_objects=dependencies)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e-T-4MO1ZR_S",
        "colab_type": "text"
      },
      "source": [
        "#RNN"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7ch0GFZ-ZSzb",
        "colab_type": "code",
        "outputId": "d21db2f0-5845-4b46-e09f-74f007fd208f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 454
        }
      },
      "source": [
        "rnn_model = Sequential()\n",
        "rnn_model.add(Embedding(max_features, embed_size))\n",
        "rnn_model.add(Bidirectional(LSTM(256, return_sequences=True, activation='tanh',input_dim=embed_size)))\n",
        "rnn_model.add(Dropout(0.2))\n",
        "rnn_model.add(Bidirectional(LSTM(256, return_sequences=True, activation='tanh')))\n",
        "rnn_model.add(GlobalMaxPool1D())\n",
        "rnn_model.add(Dropout(0.2))\n",
        "rnn_model.add(Dense(512, activation=\"relu\"))\n",
        "rnn_model.add(Dropout(0.2))\n",
        "rnn_model.add(Dense(num_classes, activation='softmax'))\n",
        "rnn_model.summary()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_1 (Embedding)      (None, None, 300)         19247100  \n",
            "_________________________________________________________________\n",
            "bidirectional (Bidirectional (None, None, 512)         1140736   \n",
            "_________________________________________________________________\n",
            "dropout_2 (Dropout)          (None, None, 512)         0         \n",
            "_________________________________________________________________\n",
            "bidirectional_1 (Bidirection (None, None, 512)         1574912   \n",
            "_________________________________________________________________\n",
            "global_max_pooling1d (Global (None, 512)               0         \n",
            "_________________________________________________________________\n",
            "dropout_3 (Dropout)          (None, 512)               0         \n",
            "_________________________________________________________________\n",
            "dense_2 (Dense)              (None, 512)               262656    \n",
            "_________________________________________________________________\n",
            "dropout_4 (Dropout)          (None, 512)               0         \n",
            "_________________________________________________________________\n",
            "dense_3 (Dense)              (None, 235)               120555    \n",
            "=================================================================\n",
            "Total params: 22,345,959\n",
            "Trainable params: 22,345,959\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C1W3XFHXZiVR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "rnn_model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy', f1, recall, precision])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tPtLB7kUrGgE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "rnn_cp_path = data_path + 'model_rnn.hdf5'\n",
        "rnn_cp=ModelCheckpoint(rnn_cp_path,monitor='val_f1',save_best_only=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2qK-tgijZk7L",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# rnn_model.fit(train_X, dummy_y_train, batch_size=512, epochs=3, \n",
        "#               validation_data=(val_X, dummy_y_val), callbacks = [rnn_cp])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4kk3g4PLUumt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "rnn_model = load_model(rnn_cp_path, custom_objects=dependencies)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dCeuUze_qnC2",
        "colab_type": "text"
      },
      "source": [
        "# Results"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3jS0Tq-Cqb4t",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "x_test['sc'] = y_test\n",
        "x_train['sc'] = y_train"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wowtwiinqwm0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Be careful! Tokenizer and maxlen is taken from train dataset from 1st part\n",
        "def convert(x):\n",
        "    train_X = tokenizer.texts_to_sequences(x.sent)\n",
        "    ## Pad the sentences \n",
        "    train_X = pad_sequences(train_X, maxlen=maxlen)\n",
        "    return train_X"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WvOMHL7Nx_eO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def metr(y_true, y_pred): #taken from old keras source code\n",
        "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
        "    possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
        "    predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
        "    precision = true_positives / (predicted_positives + K.epsilon())\n",
        "    recall = true_positives / (possible_positives + K.epsilon())\n",
        "    f1_val = 2*(precision*recall)/(precision+recall+K.epsilon())\n",
        "    return precision, recall, f1_val"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4hs6AXcPcsj1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def f1(y_true, y_pred):\n",
        "    y_pred = K.round(y_pred)\n",
        "    tp = K.sum(K.cast(y_true*y_pred, 'float'), axis=0)\n",
        "    tn = K.sum(K.cast((1-y_true)*(1-y_pred), 'float'), axis=0)\n",
        "    fp = K.sum(K.cast((1-y_true)*y_pred, 'float'), axis=0)\n",
        "    fn = K.sum(K.cast(y_true*(1-y_pred), 'float'), axis=0)\n",
        "\n",
        "    p = tp / (tp + fp + K.epsilon())\n",
        "    r = tp / (tp + fn + K.epsilon())\n",
        "\n",
        "    f1 = 2*p*r / (p+r+K.epsilon())\n",
        "    f1 = tf.where(tf.math.is_nan(f1), tf.zeros_like(f1), f1)\n",
        "    return max(p), max(r), max(f1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "adUqSHYSc2Ye",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# New metric!\n",
        "def met(y_true, y_pred):\n",
        "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)), axis=0)\n",
        "    false_positives = K.sum(K.round(K.clip((1 - y_true) * y_pred, 0, 1)), axis=0)\n",
        "    true_negatives = K.sum(K.round(K.clip((1 - y_true) * (1 - y_pred), 0, 1)), axis=0)\n",
        "    false_negatives = K.sum(K.round(K.clip(y_true * (1 - y_pred), 0, 1)), axis=0)\n",
        "    precision = max(true_positives / (true_positives + false_positives  + K.epsilon()))\n",
        "    recall = max(true_positives / (true_positives + false_negatives  + K.epsilon()))\n",
        "    f1 = 2 * precision * recall / (precision + recall + K.epsilon())\n",
        "    return precision, recall, f1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-OwT-3obvsIo",
        "colab_type": "text"
      },
      "source": [
        "# Test"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P006Z5Sgzeal",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "ps = []\n",
        "rs = []\n",
        "fs = []\n",
        "for lan in range(num_classes):\n",
        "    x_t = x_test[x_test['sc'] == lan]\n",
        "    x = convert(x_t)\n",
        "    y = np.zeros((x.shape[0], num_classes))\n",
        "    y[:, lan] = 1\n",
        "    y = y.astype('float32')\n",
        "    y_pred = cnn_model.predict(x)\n",
        "    p, r, f = met(y, y_pred)\n",
        "    ps.append(p.numpy())\n",
        "    rs.append(r.numpy())\n",
        "    fs.append(f.numpy())\n",
        "    #print(\"language: \", class_to_language[lan], \"precision: \", p.numpy(), \"recall: \", r.numpy(), \"f1: \", f.numpy())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-jFoYvQVzg0C",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "raw_metrics = pd.DataFrame(\n",
        "    {'precision': ps,\n",
        "     'recall': rs,\n",
        "     'f1': fs\n",
        "    })"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vXV6tZ4Dzh4L",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "metrics = raw_metrics.rename(class_to_language, axis='index')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3DIQwh8Z0Gq8",
        "colab_type": "code",
        "outputId": "03770912-a49d-4478-9907-24d69a55a6dd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        }
      },
      "source": [
        "print(metrics.head())"
      ],
      "execution_count": 82,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "     precision  recall        f1\n",
            "ace        1.0   0.980  0.989899\n",
            "afr        1.0   0.976  0.987854\n",
            "als        1.0   0.734  0.846597\n",
            "amh        1.0   0.986  0.992951\n",
            "ang        1.0   0.918  0.957247\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Tn_XJXIs45JJ",
        "colab_type": "code",
        "outputId": "b7e4c9b3-6859-424c-de02-78793270dd9b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 545
        }
      },
      "source": [
        "bad_quality_metrics = metrics[metrics.recall < 0.7]\n",
        "bad_quality_metrics"
      ],
      "execution_count": 87,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>precision</th>\n",
              "      <th>recall</th>\n",
              "      <th>f1</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>bel</th>\n",
              "      <td>1.0</td>\n",
              "      <td>0.490</td>\n",
              "      <td>0.657718</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>bod</th>\n",
              "      <td>1.0</td>\n",
              "      <td>0.166</td>\n",
              "      <td>0.284734</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>bos</th>\n",
              "      <td>1.0</td>\n",
              "      <td>0.540</td>\n",
              "      <td>0.701299</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>eng</th>\n",
              "      <td>1.0</td>\n",
              "      <td>0.546</td>\n",
              "      <td>0.706339</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>hbs</th>\n",
              "      <td>1.0</td>\n",
              "      <td>0.548</td>\n",
              "      <td>0.708010</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>hrv</th>\n",
              "      <td>1.0</td>\n",
              "      <td>0.592</td>\n",
              "      <td>0.743719</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>jpn</th>\n",
              "      <td>1.0</td>\n",
              "      <td>0.126</td>\n",
              "      <td>0.223801</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>khm</th>\n",
              "      <td>1.0</td>\n",
              "      <td>0.416</td>\n",
              "      <td>0.587571</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>lao</th>\n",
              "      <td>1.0</td>\n",
              "      <td>0.570</td>\n",
              "      <td>0.726115</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>lzh</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>mya</th>\n",
              "      <td>1.0</td>\n",
              "      <td>0.582</td>\n",
              "      <td>0.735777</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>spa</th>\n",
              "      <td>1.0</td>\n",
              "      <td>0.600</td>\n",
              "      <td>0.750000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>tha</th>\n",
              "      <td>1.0</td>\n",
              "      <td>0.550</td>\n",
              "      <td>0.709677</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>wuu</th>\n",
              "      <td>1.0</td>\n",
              "      <td>0.024</td>\n",
              "      <td>0.046875</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>zh-yue</th>\n",
              "      <td>1.0</td>\n",
              "      <td>0.054</td>\n",
              "      <td>0.102467</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>zho</th>\n",
              "      <td>1.0</td>\n",
              "      <td>0.056</td>\n",
              "      <td>0.106061</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "        precision  recall        f1\n",
              "bel           1.0   0.490  0.657718\n",
              "bod           1.0   0.166  0.284734\n",
              "bos           1.0   0.540  0.701299\n",
              "eng           1.0   0.546  0.706339\n",
              "hbs           1.0   0.548  0.708010\n",
              "hrv           1.0   0.592  0.743719\n",
              "jpn           1.0   0.126  0.223801\n",
              "khm           1.0   0.416  0.587571\n",
              "lao           1.0   0.570  0.726115\n",
              "lzh           0.0   0.000  0.000000\n",
              "mya           1.0   0.582  0.735777\n",
              "spa           1.0   0.600  0.750000\n",
              "tha           1.0   0.550  0.709677\n",
              "wuu           1.0   0.024  0.046875\n",
              "zh-yue        1.0   0.054  0.102467\n",
              "zho           1.0   0.056  0.106061"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 87
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zstxcPVgfUig",
        "colab_type": "code",
        "outputId": "5507b918-12b4-4630-e7af-e19a4330dc02",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 545
        }
      },
      "source": [
        "lab = labels.set_index('Label')\n",
        "bad_quality_labels = lab.loc[bad_quality_metrics.index]\n",
        "bad_quality_labels"
      ],
      "execution_count": 93,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>English</th>\n",
              "      <th>Wiki Code</th>\n",
              "      <th>ISO 369-3</th>\n",
              "      <th>Language family</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>bel</th>\n",
              "      <td>Belarusian</td>\n",
              "      <td>be</td>\n",
              "      <td>bel</td>\n",
              "      <td>Indo-European</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>bod</th>\n",
              "      <td>Tibetan</td>\n",
              "      <td>bo</td>\n",
              "      <td>bod</td>\n",
              "      <td>Sino-Tibetan</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>bos</th>\n",
              "      <td>Bosnian</td>\n",
              "      <td>bs</td>\n",
              "      <td>bos</td>\n",
              "      <td>Indo-European</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>eng</th>\n",
              "      <td>English</td>\n",
              "      <td>en</td>\n",
              "      <td>eng</td>\n",
              "      <td>Indo-European</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>hbs</th>\n",
              "      <td>Serbo-Croatian</td>\n",
              "      <td>sh</td>\n",
              "      <td>hbs</td>\n",
              "      <td>Indo-European</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>hrv</th>\n",
              "      <td>Croatian</td>\n",
              "      <td>hr</td>\n",
              "      <td>hrv</td>\n",
              "      <td>Indo-European</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>jpn</th>\n",
              "      <td>Japanese</td>\n",
              "      <td>ja</td>\n",
              "      <td>jpn</td>\n",
              "      <td>Japonic</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>khm</th>\n",
              "      <td>Central Khmer</td>\n",
              "      <td>km</td>\n",
              "      <td>khm</td>\n",
              "      <td>Austronesian</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>lao</th>\n",
              "      <td>Lao</td>\n",
              "      <td>lo</td>\n",
              "      <td>lao</td>\n",
              "      <td>Tai-Kadai</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>lzh</th>\n",
              "      <td>Literary Chinese</td>\n",
              "      <td>zh-classical</td>\n",
              "      <td>lzh</td>\n",
              "      <td>Sino-Tibetan</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>mya</th>\n",
              "      <td>Burmese</td>\n",
              "      <td>my</td>\n",
              "      <td>mya</td>\n",
              "      <td>Sino-Tibetan</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>spa</th>\n",
              "      <td>Spanish</td>\n",
              "      <td>es</td>\n",
              "      <td>spa</td>\n",
              "      <td>Indo-European</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>tha</th>\n",
              "      <td>Thai</td>\n",
              "      <td>th</td>\n",
              "      <td>tha</td>\n",
              "      <td>Tai-Kadai</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>wuu</th>\n",
              "      <td>Wu Chinese</td>\n",
              "      <td>wuu</td>\n",
              "      <td>wuu</td>\n",
              "      <td>Sino-Tibetan</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>zh-yue</th>\n",
              "      <td>Cantonese</td>\n",
              "      <td>zh-yue</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Sino-Tibetan</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>zho</th>\n",
              "      <td>Standard Chinese</td>\n",
              "      <td>zh</td>\n",
              "      <td>zho</td>\n",
              "      <td>Sino-Tibetan</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                 English     Wiki Code ISO 369-3 Language family\n",
              "bel           Belarusian            be       bel   Indo-European\n",
              "bod              Tibetan            bo       bod    Sino-Tibetan\n",
              "bos              Bosnian            bs       bos   Indo-European\n",
              "eng              English            en       eng   Indo-European\n",
              "hbs       Serbo-Croatian            sh       hbs   Indo-European\n",
              "hrv             Croatian            hr       hrv   Indo-European\n",
              "jpn             Japanese            ja       jpn         Japonic\n",
              "khm        Central Khmer            km       khm    Austronesian\n",
              "lao                  Lao            lo       lao       Tai-Kadai\n",
              "lzh     Literary Chinese  zh-classical       lzh    Sino-Tibetan\n",
              "mya              Burmese            my       mya    Sino-Tibetan\n",
              "spa              Spanish            es       spa   Indo-European\n",
              "tha                 Thai            th       tha       Tai-Kadai\n",
              "wuu           Wu Chinese           wuu       wuu    Sino-Tibetan\n",
              "zh-yue         Cantonese        zh-yue       NaN    Sino-Tibetan\n",
              "zho     Standard Chinese            zh       zho    Sino-Tibetan"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 93
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gUkII9aS9iK1",
        "colab_type": "code",
        "outputId": "4c2bf3e3-915c-4cd7-bb1c-584d464d0d7e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 487
        }
      },
      "source": [
        "print(x_train[x_train.sc == language_to_class['wuu']])\n",
        "print(x_train[x_train.sc == language_to_class['lzh']])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "                                                     sent   sc\n",
            "19      UNC有得一只历史悠久个'诚信守则'。渠是由学堂个诚信法庭（Honor Court）来执行个...  187\n",
            "276     武器，是人类为达到杀伤或者防御个目的制造个器械。武器从人类文明发展开始就有出现，之后，伴随战...  187\n",
            "387     弗过，太后个信任是一方面，具体西班牙个政治局面又是另外一番情形：贵族对迭个外国人一点也无信任...  187\n",
            "720     从12世纪挨末阶段开始，日本个统治权就转移到日本武士贵族个手里向。到13世纪，出身清和源氏个...  187\n",
            "930     箇种毛病潜伏期一般勒7日天以内，病人一般表现为流感箇浪个症状，像发寒热，咳嗽，少痰，有种辰光...  187\n",
            "...                                                   ...  ...\n",
            "116805  Linux個低成本、強大個定制功能搭良好個移植性能，使得Linux來嵌入式系統方面也得到廣泛...  187\n",
            "116947  余杭區勒拉杭嘉湖平原南端，西依天目山，南瀕錢塘江，是長江三角洲个圓心地。地理座標爲北緯30°...  187\n",
            "117034  北師大是以京師大學堂師範專業做基礎，搭仔由北京個高校匯聚一批勒師範教育領域窮有聲望個名師組建...  187\n",
            "117037  到清中期（約18世紀），傳奇開始衰微，向書齋文學轉化，彈詞卻逐步興盛起來了。出版之錢德蒼編个...  187\n",
            "117257  摩嘉娜夺权计划失败后，带领奄奄一息个莫高絲逃出卡美洛特，徕萨温节夜里，摩嘉娜徕莫高絲个要求之...  187\n",
            "\n",
            "[500 rows x 2 columns]\n",
            "                                                     sent  sc\n",
            "336     武漢市，亦稱以漢，乃中華鄂省之會，亦為七大都市於中華之中原也。方八千四百六十七公里，於西元二...  97\n",
            "420     按黃帝為法，數有十等。及其用也，乃有三焉。十等者，謂「億、兆、京、垓、秭、壤、溝、澗、正、載...  97\n",
            "1254    范蠡浮海出齊，變姓名，自謂鴟夷子皮，耕於海畔，苦身戮力，父子治產。居無幾何，致產數千萬。齊人...  97\n",
            "1414    周迪據臨川反，詔昭達便道征之。迪敗走，徵為護軍將軍，改封邵武縣侯。四年，陳寶應納迪，共寇臨川...  97\n",
            "1549    喇克達長子敬德，康熙十一年(一六七二年)封三等奉恩將軍，四十七年(一七零八年)卒。敬德次子班...  97\n",
            "...                                                   ...  ..\n",
            "116047  士族者，或曰門第、衣冠、世族、門閥、勢族、世家、巨室。蓋謂世居要位之高門也，世族所居不同，中...  97\n",
            "116419  淳化二年秋七月，李繼遷請降，以爲銀州觀察使，賜姓名趙保吉。繼捧至夏州數月，即言繼遷悔過歸款，...  97\n",
            "116596  孟嘗君怨秦，將以齊為韓、魏攻楚，因與韓、魏攻秦，而借兵食於西周。蘇代為西周謂曰：「君以齊為韓...  97\n",
            "116749  陳敏之亂，弘以侃為江夏太守，加鷹揚將軍。侃備威儀，迎母官舍，鄉裡榮之。敏遣其弟恢來寇武昌，侃...  97\n",
            "117497  同年，太后崩。絳侯周勃、陳平諸臣共謀誅呂。朱虛侯章已殺呂產，文帝使人持節勞章。朱虛侯欲奪節信...  97\n",
            "\n",
            "[500 rows x 2 columns]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dH862g-VYj8J",
        "colab_type": "text"
      },
      "source": [
        "# Fix Chinese langs"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yltabFr0UYF_",
        "colab_type": "code",
        "outputId": "34d578ab-68f4-4cd7-cc38-d08d162c7838",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# Extract Unigrams\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "unigramVectorizer = CountVectorizer(analyzer='char', ngram_range=(1,1))\n",
        "X_unigram_train_raw = unigramVectorizer.fit_transform(x_train)\n",
        "X_unigram_test_raw = unigramVectorizer.transform(x_test)\n",
        "\n",
        "\n",
        "unigramFeatures = unigramVectorizer.get_feature_names()\n",
        "\n",
        "print('Number of unigrams in training set:', len(unigramFeatures))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of unigrams in training set: 4\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KXhcTGMaYZXt",
        "colab_type": "code",
        "outputId": "f449f432-dd28-40df-fdf4-863b20e062f0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 346
        }
      },
      "source": [
        "def train_lang_dict(X_raw_counts, y_train):\n",
        "    lang_dict = {}\n",
        "    for i in range(len(y_train)):\n",
        "        lang = y_train[i]\n",
        "        v = np.array(X_raw_counts[i])\n",
        "        if not lang in lang_dict:\n",
        "            lang_dict[lang] = v\n",
        "        else:\n",
        "            lang_dict[lang] += v\n",
        "            \n",
        "    # to relative\n",
        "    for lang in lang_dict:\n",
        "        v = lang_dict[lang]\n",
        "        lang_dict[lang] = v / np.sum(v)\n",
        "        \n",
        "    return lang_dict\n",
        "\n",
        "language_dict_unigram = train_lang_dict(X_unigram_train_raw.toarray(), y_train.values)\n",
        "\n",
        "# Collect relevant chars per language\n",
        "def getRelevantCharsPerLanguage(features, language_dict, significance=1e-5):\n",
        "    relevantCharsPerLanguage = {}\n",
        "    for lang in languages:\n",
        "        chars = []\n",
        "        relevantCharsPerLanguage[lang] = chars\n",
        "        v = language_dict[lang]\n",
        "        for i in range(len(v)):\n",
        "            if v[i] > significance:\n",
        "                chars.append(features[i])\n",
        "    return relevantCharsPerLanguage\n",
        "\n",
        "relevantCharsPerLanguage = getRelevantCharsPerLanguage(unigramFeatures, language_dict_unigram)\n",
        "    \n",
        "# Print number of unigrams per language\n",
        "for lang in languages:    \n",
        "    print(lang, len(relevantCharsPerLanguage[lang]))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-61-592510daad2e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mlang_dict\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m \u001b[0mlanguage_dict_unigram\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_lang_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_unigram_train_raw\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;31m# Collect relevant chars per language\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-61-592510daad2e>\u001b[0m in \u001b[0;36mtrain_lang_dict\u001b[0;34m(X_raw_counts, y_train)\u001b[0m\n\u001b[1;32m      4\u001b[0m         \u001b[0mlang\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[0mv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_raw_counts\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mlang\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mlang_dict\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m             \u001b[0mlang_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlang\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mv\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: unhashable type: 'numpy.ndarray'"
          ]
        }
      ]
    }
  ]
}